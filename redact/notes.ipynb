{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c52616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/redact/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ PyTorch version: 2.7.1+cu128\n",
      "ü§ó Transformers version: 4.54.0\n",
      "üìä TRL version: 0.20.0\n",
      "Is cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import trl\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import Optional\n",
    "import re\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "print(f\"üì¶ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ó Transformers version: {transformers.__version__}\")\n",
    "print(f\"üìä TRL version: {trl.__version__}\")\n",
    "print(f\"Is cuda available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc08511c-3e4b-4a09-aaea-2e63850cb02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Loading tokenizer...\n",
      "üß† Loading model...\n",
      "‚úÖ Local model loaded successfully!\n",
      "üî¢ Parameters: 1,170,340,608\n",
      "üìñ Vocab size: 64400\n",
      "üíæ Model size: ~2.3 GB (bfloat16)\n",
      "Model device: cuda:0\n",
      "model.training = False\n",
      "Padding side: left\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# https://huggingface.co/LiquidAI/LFM2-1.2B\n",
    "# model_id = \"LiquidAI/LFM2-1.2B\"\n",
    "\n",
    "\n",
    "# https://huggingface.co/LiquidAI/LFM2-1.2B-Extract\n",
    "model_id = \"LiquidAI/LFM2-1.2B-Extract\"\n",
    "\n",
    "\n",
    "print(\"üìö Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# IMPORTANT: Set left padding for decoder-only models\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"üß† Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    # attn_implementation=\"flash_attention_2\" # <- uncomment on compatible GPU\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Local model loaded successfully!\")\n",
    "print(f\"üî¢ Parameters: {model.num_parameters():,}\")\n",
    "print(f\"üìñ Vocab size: {len(tokenizer)}\")\n",
    "print(f\"üíæ Model size: ~{model.num_parameters() * 2 / 1e9:.1f} GB (bfloat16)\")\n",
    "print(f\"Model device: {model.device}\")\n",
    "print(f\"model.training = {model.training}\")\n",
    "print(f\"Padding side: {tokenizer.padding_side}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ac57842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/redact/.venv/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/redact/.venv/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkainoj2\u001b[0m (\u001b[33mkainoj2-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/redact/wandb/run-20251012_051312-wlipbboi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kainoj2-none/liquid-ai/runs/wlipbboi' target=\"_blank\">LiquidAI-LFM2-1.2B-Extract-run-przm043</a></strong> to <a href='https://wandb.ai/kainoj2-none/liquid-ai' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kainoj2-none/liquid-ai' target=\"_blank\">https://wandb.ai/kainoj2-none/liquid-ai</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kainoj2-none/liquid-ai/runs/wlipbboi' target=\"_blank\">https://wandb.ai/kainoj2-none/liquid-ai/runs/wlipbboi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "RUN_NAME = f\"{model_id.replace('/', '-')}-run-przm043\"\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the wandb entity where your project will be logged (generally your team name).\n",
    "    entity=\"kainoj2-none\",\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    project=\"liquid-ai\",\n",
    "    name=RUN_NAME,\n",
    "    config={\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"lr_scheduler_type\": \"linear\",\n",
    "        \"warmup_steps\": 100,\n",
    "        \"warmup_ratio\": 0.2,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d629c6d",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06c0f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"Identify and extract information matching the following schema.\n",
    "Return data as a JSON object. \n",
    "For each field, select most suitable value from text\n",
    "If provided text does not contain sufficient information to fill out the field, make the field empty string.\n",
    "Output only JSON, and output only four fields. \n",
    "\n",
    "{\n",
    "    \"full_name\": \"name of the person\",\n",
    "    \"company_name\": \"name of the company\",\n",
    "    \"address\": \"address of the plance\",\n",
    "    \"phone_number\": \"phone number\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def promptify(example):\n",
    "   \n",
    "    text = example['text']\n",
    "    json_label = example['json']\n",
    "\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        \"completion\": [\n",
    "            {\"role\": \"assistant\", \"content\": json_label}\n",
    "        ],\n",
    "    }\n",
    "\n",
    "os.makedirs(f\"./models/{RUN_NAME}\", exist_ok=True)\n",
    "with open(f\"./models/{RUN_NAME}/system_prompt.txt\", \"w+\") as f:\n",
    "    f.write(SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3f100",
   "metadata": {},
   "source": [
    "### Their dataset \n",
    "https://github.com/stockmarkteam/ner-wikipedia-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed21281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['entities', 'text', 'curid', 'address', 'full_name', 'company_name', 'json', 'prompt', 'completion'],\n",
       "     num_rows: 800\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['entities', 'text', 'curid', 'address', 'full_name', 'company_name', 'json', 'prompt', 'completion'],\n",
       "     num_rows: 200\n",
       " }))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from their_dataset import load_their_dataset, add_json_label\n",
    "\n",
    "# ds = load_their_dataset().map(add_json_label).map(promptify)\n",
    "# ds_splits = ds[\"train\"].train_test_split(test_size=1000, seed=42) \n",
    "\n",
    "# For testing\n",
    "ds = load_their_dataset().map(add_json_label).map(promptify)\n",
    "ds_splits = ds[\"train\"].select(range(1000)).train_test_split(test_size=0.2, seed=42) \n",
    "\n",
    "ds_train = ds_splits['train']\n",
    "ds_eval = ds_splits['test']\n",
    "\n",
    "ds_train, ds_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0798486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:00<00:00, 10100.29 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:00<00:00, 9068.46 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file_name', 'full_name', 'company_name', 'address', 'phone_number', 'template_id', 'text', 'json', 'prompt', 'completion'],\n",
       "    num_rows: 64\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from our_dataset import load_our_dataset\n",
    "\n",
    "our_ds = load_our_dataset().map(add_json_label).map(promptify)\n",
    "# our_ds_splits = our_ds[\"train\"].train_test_split(test_size=32, seed=42)\n",
    "\n",
    "\n",
    "#our_ds_train = our_ds_splits['train']\n",
    "our_ds_eval = our_ds['train']\n",
    "\n",
    "our_ds_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edd9116",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d0293d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(llm_output: str) -> Optional[dict]:\n",
    "    if llm_output is None:\n",
    "        return None\n",
    "    try:\n",
    "        result = json.loads(llm_output.strip())\n",
    "        return result if isinstance(result, dict) else None\n",
    "    except (json.JSONDecodeError, ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def has_same_fields(generated: dict, golden: dict) -> int:\n",
    "    \"\"\"Return 1 if `generated` has exactly the same keys as `golden`, else 0.\"\"\"\n",
    "    if generated is None or golden is None:\n",
    "        return 0\n",
    "    \n",
    "    return int(set(generated.keys()) == set(golden.keys()))\n",
    "\n",
    "\n",
    "def get_gold(ex):\n",
    "    label_dict = {\n",
    "        \"full_name\": ex.get('full_name', ''),\n",
    "        \"company_name\": ex.get('company_name', ''),\n",
    "        \"address\": ex.get('address', ''),\n",
    "        \"phone_number\": ex.get('phone_number', '')\n",
    "    }\n",
    "    return label_dict\n",
    "\n",
    "\n",
    "def extract_assistant_response(decoded_text: str) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Extract and parse assistant response from chat template format.\n",
    "    \n",
    "    Expected format:\n",
    "    <|im_start|>assistant\n",
    "    {\"field1\": \"value1\", \"field2\": \"value2\"}<|im_end|>\n",
    "    \n",
    "    Returns string\n",
    "    \"\"\"\n",
    "    # Extract content between assistant tags\n",
    "    pattern = r'<\\|im_start\\|>assistant\\s*(.*?)<\\|im_end\\|>'\n",
    "    match = re.search(pattern, decoded_text, re.DOTALL)\n",
    "\n",
    "    if not match:\n",
    "        return None\n",
    "        # raise Exception(f\"fuck assistant response shall not be empty. decoded_text = {decoded_text}\")\n",
    "\n",
    "    assistant_content = match.group(1).strip()\n",
    "    return assistant_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaa1c70",
   "metadata": {},
   "source": [
    "# Process one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe32ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one(sample):\n",
    "\n",
    "    # sanity check\n",
    "    assert isinstance(sample, dict)\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        sample['prompt'],\n",
    "        add_generation_prompt=True, #### Set false for training\n",
    "        return_tensors=\"pt\",\n",
    "        tokenize=True, # !!! true=> tokens, false=> text\n",
    "    )\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids.to(model.device),\n",
    "        do_sample=True,\n",
    "        temperature=0.3,\n",
    "        min_p=0.15,\n",
    "        repetition_penalty=1.05,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "\n",
    "    ## it's always output[0]\n",
    "    generated_ids = output[0][input_ids.shape[-1]:]  # Skip the input tokens\n",
    "    decoded = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    # returns decoded assistant answers\n",
    "    return decoded\n",
    "\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for sample in ds['train'].shuffle(seed=43):\n",
    "    total += 1\n",
    "    decoded = process_one(sample)\n",
    "    ans = has_same_fields(get_json(decoded), get_gold(sample))\n",
    "    if (ans == 0):\n",
    "        print(get_gold(sample), decoded)\n",
    "        print()\n",
    "    correct += ans\n",
    "    if total % 10:\n",
    "        print(correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0eee1",
   "metadata": {},
   "source": [
    "# Eval batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "638d408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader that:\n",
    "    - Takes a list of dataset samples\n",
    "    - Applies chat template to each\n",
    "    - Pads to same length\n",
    "    - Returns batch dict\n",
    "    \"\"\"\n",
    "    prompts = [sample['prompt'] for sample in batch]\n",
    "    \n",
    "    # Apply chat template with padding to entire batch\n",
    "    encoded = tokenizer.apply_chat_template(\n",
    "        prompts,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,  # Pad to longest sequence in batch\n",
    "        tokenize=True,\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'input_ids': encoded,\n",
    "        'samples': batch  # Keep original samples for gold labels\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_batched(model_eval, evaluation_dataset, batch_size = 128):\n",
    "    # Create DataLoader with custom collate function\n",
    "    dataloader = DataLoader(\n",
    "        evaluation_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # Keep order for evaluation\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "    )\n",
    "\n",
    "    # Set model to eval mode\n",
    "    model_eval.eval()\n",
    "\n",
    "    correct_answs = 0\n",
    "    total = 0\n",
    "\n",
    "    failed_stuff = []\n",
    "\n",
    "    for batch_dict in tqdm(dataloader, desc=\"Evaluating batches\"):\n",
    "        # Get predictions for entire batch\n",
    "        input_ids = batch_dict['input_ids'].to(model.device)\n",
    "    \n",
    "        # Generate for entire batch at once\n",
    "        outputs = model_eval.generate(\n",
    "            input_ids,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            min_p=0.15,\n",
    "            repetition_penalty=1.05,\n",
    "            max_new_tokens=512,\n",
    "        )\n",
    "        \n",
    "        # Decode each sequence one by one\n",
    "        predictions = []\n",
    "        \n",
    "        for output_seq in outputs:\n",
    "            decoded = tokenizer.decode(output_seq, skip_special_tokens=False)\n",
    "            extracted_assistant = extract_assistant_response(decoded)\n",
    "            predictions.append(extracted_assistant)\n",
    "\n",
    "        # Score each prediction against gold label\n",
    "        for pred_raw, sample in zip(predictions, batch_dict['samples']):\n",
    "            gold = get_gold(sample)\n",
    "            pred = get_json(pred_raw)\n",
    "            has_same = has_same_fields(pred, gold)\n",
    "            if (has_same==0):\n",
    "                failed_stuff.append({\n",
    "                    \"gold\": gold,\n",
    "                    \"pred\": pred,\n",
    "                    \"pred_raw\": pred_raw\n",
    "                })\n",
    "            \n",
    "            correct_answs += has_same\n",
    "            total += 1\n",
    "\n",
    "        # Print progress every batch_size samples\n",
    "        if total % batch_size == 0:\n",
    "            print(f\"Progress: {total} samples, Accuracy so far: {correct_answs / total:.4f}\")\n",
    "\n",
    "    acc = correct_answs /  total\n",
    "    print(f\"\\n‚úÖ Final Accuracy: {acc:.4f}\")\n",
    "    print(f\"üìä Correct: {correct_answs}/{total}\")\n",
    "\n",
    "    return acc, failed_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41f2e4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating batches:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:11<00:11, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 128 samples, Accuracy so far: 0.8906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:21<00:00, 10.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Final Accuracy: 0.9100\n",
      "üìä Correct: 182/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc_raw_model_their, failed_stuff_theirs = eval_batched(model, evaluation_dataset=ds_eval, batch_size=128)\n",
    "wandb.log({\"accuracy_raw_model/theirs_simple_dataset\": acc_raw_model_their})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0f24cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:10<00:00, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Final Accuracy: 0.5781\n",
      "üìä Correct: 37/64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc_raw_model_ours, failed_stuff_ours = eval_batched(model, evaluation_dataset=our_ds_eval, batch_size=128)\n",
    "wandb.log({\"accuracy_raw_model/our_complex_dataset\": acc_raw_model_ours})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "082d0fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'full_name': 'Áü≥Â∑ù Â§ßÂú∞', 'company_name': 'Èñ¢Ë•ø„Çπ„ÉÅ„Éº„É´Ê†™Âºè‰ºöÁ§æ', 'address': '„Äí542-0076 Â§ßÈò™Â∫úÂ§ßÈò™Â∏Ç‰∏≠Â§ÆÂå∫Èõ£Ê≥¢5-1-60 „Å™„Çì„Å∞„Éë„Éº„ÇØ„Çπ', 'phone_number': '06-6644-7102'}\n",
      "{\n",
      "  \"Â•ëÁ¥ÑÁ∑†ÁµêÂπ¥ÊúàÊó•\": \"‰ª§Âíå7Âπ¥10Êúà11Êó•\",\n",
      "  \"Âä¥ÂÉçÊù°‰ª∂ÈÄöÁü•Êõ∏\": \"Ê†™Âºè‰ºöÁ§æÁü≥Â∑ùÂ§ßÂú∞\",\n",
      "  \"‰ΩèÊâÄ\": \"Â§ßÈò™Â∫úÂ§ßÈò™Â∏Ç‰∏≠Â§ÆÂå∫Èõ£Ê≥¢ 5-1-60„Å™„Çì„Å∞„Éë„Éº„ÇØ„Çπ\",\n",
      "  \"ÈõªË©±Áï™Âè∑\": \"06-6644-7102\",\n",
      "  \"Â•ëÁ¥ÑÊúüÈñì\": \"ÂÆö„ÇÅÁÑ°„Åó\",\n",
      "  \"Ë©¶Áî®ÊúüÈñì\": \"Ôºì„ÅãÊúà\",\n",
      "  \"Â∞±Ê•≠„ÅÆÂ†¥ÊâÄ\": \"Êú¨Á§æ\",\n",
      "  \"Â∞ÜÊù•Êã†ÁÇπÁï∞Âãï„ÅÆÂèØËÉΩÊÄß„ÅÇ„Çä\": \"Âæì‰∫ã„Åô„Åπ„ÅçÊ•≠Âãô\",\n",
      "  \"Ê•≠ÂãôÂÜÖÂÆπ\": \"Áî£Ê•≠Áî®„É≠„Éú„ÉÉ„Éà„ÅÆÁµÑÁ´ã„ÉªÊ§úÊüª„ÉªÂ∑•Á®ãÊîπÂñÑË£úÂä©\",\n",
      "  \"‰ºëÊÜ©ÊôÇÈñì\": \"9:00\",\n",
      "  \"‰ºëÊÜ©ÂàÜ\": \"60ÂàÜ\",\n",
      "  \"‰ºëÊó•\": \"Âúü„ÉªÊó•„ÉªÁ•ù„ÉªÂπ¥Êú´Âπ¥Âßã12/29„Äú1/3\",\n",
      "  \"Âπ¥Èñì‰ºëÊöá\": \"120Êó•\",\n",
      "  \"Âπ¥‰ºëÊ≥ïÂÆö\": \"ÂàùÂπ¥Â∫¶10Êó•Á∂ôÁ∂öÂã§Âãô Ôºñ„ÅãÊúà‰ª•ÂÜÖ„ÅÆÂπ¥‰ºëÊúâÂü∫Êú¨Ë≥ÉÈáëÊúàÁµ¶330,000ÂÜÜË´∏ÊâãÂΩìÈÄöÂã§ÂÆüË≤ªC‰∏äÈôê30,000ÂÜÜ„ÄÅ‰ΩèÂÆÖÊâãÂΩì 10,000ÂÜÜÂâ≤Â¢óË≥ÉÈáëÁéáÊ≥ïÂÆöË∂Ö 25ÔºÖÊ≥ïÂÆö‰ºëÊó•35%Ê∑±Â§ú 25ÔºÖ„ÄÅÊúà60hË∂Ö 50ÔºÖË≥ÉÈáëÁ∑†Âàá„ÉªÊîØÊâïÁ∑†ÂàáÔºöÊØéÊúàÊú´Êó•„ÄÅÊîØÊâïÔºöÁøåÊúà25Êó•ÔºàÂè£Â∫ßÊåØËæº)ÊéßÈô§Êúâ\",\n",
      "  \"ÊòáÁµ¶„ÉªË≥û‰∏é„ÉªÈÄÄËÅ∑Èáë\": \"Âπ¥1Âõû„ÄÅË≥û‰∏éÔºöÂπ¥ÔºíÂõûÈÄÄËÅ∑ÈáëÔºöÁÑ°ÈÄÄËÅ∑ÔºöÂÆöÂπ¥ 60Ê≠≥„ÄÄÁ∂ôÁ∂öÈõáÁî® 65 Ê≠≥„Åæ„Åß\",\n",
      "  \"Ëá™Â∑±ÈÉΩÂêà\": \"30Êó•ÂâçÂ±äÂá∫Ëß£ÈõáÂ∞±Ê•≠Ë¶èÂâáÈÜíÁî®Áõ∏Ë´áÁ™ìÂè£‰∫∫‰∫ãÁ∑èÂãôÈÉ®\",\n",
      "  \"Âè£Â∫ßÊåØËæº\": \"ÊéßÈô§Êúâ\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 6\n",
    "print(failed_stuff_ours[sample_idx]['gold'])\n",
    "print(failed_stuff_ours[sample_idx]['pred_raw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6053779",
   "metadata": {},
   "source": [
    "# Lora SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d7a8dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,554,176 || all params: 1,175,894,784 || trainable%: 0.4723\n",
      "‚úÖ LoRA configuration applied!\n",
      "üéõÔ∏è  LoRA rank: 8\n",
      "üìä LoRA alpha: 16\n",
      "üéØ Target modules: {'w2', 'in_proj', 'q_proj', 'w1', 'w3', 'v_proj', 'out_proj', 'k_proj'}\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "GLU_MODULES = [\"w1\", \"w2\", \"w3\"]\n",
    "MHA_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
    "CONV_MODULES = [\"in_proj\", \"out_proj\"]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,  # <- lower values = fewer parameters\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=GLU_MODULES + MHA_MODULES + CONV_MODULES,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=None,\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "print(\"‚úÖ LoRA configuration applied!\")\n",
    "print(f\"üéõÔ∏è  LoRA rank: {lora_config.r}\")\n",
    "print(f\"üìä LoRA alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"üéØ Target modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc58c10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è  Creating LoRA SFT trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:00<00:00, 1175.38 examples/s]\n",
      "Truncating train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [00:00<00:00, 121087.05 examples/s]\n",
      "Tokenizing eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 1111.89 examples/s]\n",
      "Truncating eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 37429.09 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting LoRA + SFT training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [800/800 01:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>0.059719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ LoRA + SFT training completed!\n",
      "üíæ LoRA model saved to: ./models/LiquidAI-LFM2-1.2B-Extract-run-przm043/lfm2-sft-lora/\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "\n",
    "lora_sft_config = SFTConfig(\n",
    "    output_dir=f\"./models/{RUN_NAME}/lfm2-sft-lora/\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=wandb.config['learning_rate'],\n",
    "    lr_scheduler_type=wandb.config['lr_scheduler_type'],\n",
    "    warmup_steps=wandb.config['warmup_steps'],\n",
    "    warmup_ratio=wandb.config['warmup_ratio'],\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "print(\"üèóÔ∏è  Creating LoRA SFT trainer...\")\n",
    "lora_sft_trainer = SFTTrainer(\n",
    "    model=lora_model,\n",
    "    args=lora_sft_config,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_eval,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Starting LoRA + SFT training...\")\n",
    "lora_sft_trainer.train()\n",
    "\n",
    "print(\"üéâ LoRA + SFT training completed!\")\n",
    "\n",
    "lora_sft_trainer.save_model()\n",
    "print(f\"üíæ LoRA model saved to: {lora_sft_config.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e12fc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Merging LoRA weights...\n",
      "üíæ Merged model saved to: ./models/LiquidAI-LFM2-1.2B-Extract-run-przm043/lfm2-lora-merged\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîÑ Merging LoRA weights...\")\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "merged_output_dir = f\"./models/{RUN_NAME}/lfm2-lora-merged\"\n",
    "merged_model.save_pretrained(merged_output_dir)\n",
    "tokenizer.save_pretrained(merged_output_dir)\n",
    "print(f\"üíæ Merged model saved to: {merged_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c2e20a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating batches:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 128 samples, Accuracy so far: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Final Accuracy: 1.0000\n",
      "üìä Correct: 200/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc_finetuned_theirs, failed_examples_post_theirs = eval_batched(merged_model, ds_eval, 128)\n",
    "wandb.log({\"accuracy_finetuned_model/theirs_simple_dataset\": acc_finetuned_theirs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b3fe4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:09<00:00, 10.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Final Accuracy: 0.9688\n",
      "üìä Correct: 62/64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc_finetuned_ours, failed_examples_post_ours = eval_batched(merged_model, our_ds_eval, 128)\n",
    "wandb.log({\"accuracy_finetuned_model/our_complex_dataset\": acc_finetuned_theirs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a43a4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'full_name': 'Â§ßÂ°ö „Åï„Åè„Çâ', 'company_name': 'Ê†™Âºè‰ºöÁ§æ„Éç„Ç™„Éª„Ç≥„Éº„Éù„É¨„Éº„Ç∑„Éß„É≥', 'address': '„Äí060-0005 ÂåóÊµ∑ÈÅìÊú≠ÂπåÂ∏Ç‰∏≠Â§ÆÂå∫Âåó5Êù°Ë•ø2-5 JR„Çø„ÉØ„Éº', 'phone_number': '011-209-5100'}\n",
      "None\n",
      "=====\n",
      "{'full_name': 'Á¶èÁî∞ Èöº‰∫∫', 'company_name': '„Ç™„Éº„Ç∑„É£„É≥„Éª„Ç≠„É£„Éî„Çø„É´„Éª„Éë„Éº„Éà„Éä„Éº„Ç∫', 'address': '„Äí100-6990 Êù±‰∫¨ÈÉΩÂçÉ‰ª£Áî∞Âå∫‰∏∏„ÅÆÂÜÖ2-6-1 ‰∏∏„ÅÆÂÜÖ„Éñ„É™„ÉÉ„ÇØ„Çπ„ÇØ„Ç®„Ç¢', 'phone_number': '03-3211-8800'}\n",
      "{'„Éï„É´„Éç„Éº„É†': 'Á¶èÁî∞Èöº‰∫∫', '‰ºöÁ§æÂêç': '„Ç™„Éº„Ç∑„É£„É≥„Éª„Ç≠„É£„Éî„Çø„É´„Éª„Éë„Éº„Éà„Éä„Éº„Ç∫', '‰ΩèÊâÄ': 'Êù±‰∫¨ÈÉΩÂçÉ‰ª£Áî∞Âå∫‰∏∏„ÅÆÂÜÖ2-6-1‰∏∏„ÅÆÂÜÖ„Éñ„É™„ÉÉ„ÇØ„Çπ„ÇØ„Ç®„Ç¢', 'ÈõªË©±Áï™Âè∑': '03-3211-8800'}\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for f in failed_examples_post_ours:\n",
    "    print(f['gold'])\n",
    "    print(f['pred'])\n",
    "    print(\"=====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39d4d1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (1 / 1): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.34GB / 2.34GB,  273MB/s  \n",
      "New Data Upload: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.09GB / 2.09GB,  248MB/s  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kainoj/LiquidAI-LFM2-1.2B-Extract-ja-pii-finetuned/commit/df47251bde078eb7a48763d4ef5e511b8730db13', commit_message='Upload tokenizer', commit_description='', oid='df47251bde078eb7a48763d4ef5e511b8730db13', pr_url=None, repo_url=RepoUrl('https://huggingface.co/kainoj/LiquidAI-LFM2-1.2B-Extract-ja-pii-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='kainoj/LiquidAI-LFM2-1.2B-Extract-ja-pii-finetuned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"kainoj/LiquidAI-LFM2-1.2B-Extract-ja-pii-finetuned\"\n",
    "\n",
    "# One line each:\n",
    "merged_model.push_to_hub(repo_id)\n",
    "tokenizer.push_to_hub(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad6a1ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"system_prompt\": SYSTEM_PROMPT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8a807fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_table = wandb.Table(columns=[\"System Prompt\"])\n",
    "text_table.add_data(SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c6e1432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kainoj/LiquidAI-LFM2-1.2B-Extract-ja-pii-finetuned/commit/7957179faff0048bde2f853f50004e3a633c4de8', commit_message='Upload LICENSE with huggingface_hub', commit_description='', oid='7957179faff0048bde2f853f50004e3a633c4de8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/kainoj/LiquidAI-LFM2-1.2B-Extract-ja-pii-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='kainoj/LiquidAI-LFM2-1.2B-Extract-ja-pii-finetuned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, upload_file\n",
    "\n",
    "api = HfApi()\n",
    "upload_file(\n",
    "    path_or_fileobj=\"models/LICENSE\",        # local file path\n",
    "    path_in_repo=\"LICENSE\",           # destination name in repo\n",
    "    repo_id=repo_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19222c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
