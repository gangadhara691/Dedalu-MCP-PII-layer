{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T23:17:12.057364Z",
     "start_time": "2023-10-18T23:17:11.590192Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-27T02:10:14.706649Z",
     "iopub.status.busy": "2025-03-27T02:10:14.705816Z",
     "iopub.status.idle": "2025-03-27T02:10:15.072020Z",
     "shell.execute_reply": "2025-03-27T02:10:15.071682Z",
     "shell.execute_reply.started": "2025-03-27T02:10:14.706550Z"
    },
    "nbdime-conflicts": {
     "local_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2022-12-26T05:43:42.405443Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2022-12-26T05:43:42.092571Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ],
     "remote_diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-01-05T00:00:41.096307Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "end_time",
         "op": "patch"
        },
        {
         "diff": [
          {
           "key": 0,
           "op": "addrange",
           "valuelist": [
            "2023-01-05T00:00:40.908324Z"
           ]
          },
          {
           "key": 0,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": "start_time",
         "op": "patch"
        }
       ],
       "key": "ExecuteTime",
       "op": "patch"
      }
     ]
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Hugging face (slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "# # Load model and tokenizer\n",
    "# # model_id = \"LiquidAI/LFM2-1.2B\" # <- or LFM2-700M or LFM2-350M\n",
    "# # model_id = \"LiquidAI/LFM2-700M\"\n",
    "# model_id = \"LiquidAI/LFM2-350M-ENJP-MT\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     device_map=\"auto\",\n",
    "#     dtype=\"bfloat16\",\n",
    "# #   attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# # streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# def fluid_translate(prompt, lang='jp2en'):\n",
    "#     if lang == \"jp2en\":\n",
    "#         system_prompt = \"Translate to English\"\n",
    "#     elif lang == \"en2jp\":\n",
    "#         system_prompt = \"Translate to Japanese\"\n",
    "#     else:\n",
    "#         raise Exception(\"lang not supported\")\n",
    "\n",
    "#     input_ids = tokenizer.apply_chat_template(\n",
    "#         [{\"role\": \"system\", \"content\": system_prompt },\n",
    "#         {\"role\": \"user\", \"content\": prompt}],\n",
    "#         add_generation_prompt=True,\n",
    "#         return_tensors=\"pt\",\n",
    "#         tokenize=True,\n",
    "#     ).to(model.device)\n",
    "\n",
    "#     output = model.generate(\n",
    "#         input_ids,\n",
    "#         do_sample=True,\n",
    "#         temperature=0.1,\n",
    "#         min_p=0.15,\n",
    "#         repetition_penalty=1.05,\n",
    "#         max_new_tokens=16000,\n",
    "#         # streamer=streamer,\n",
    "#     )\n",
    "#     # Decode the output to get the text\n",
    "#     full_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "#     # Extract only the assistant's response\n",
    "#     if \"assistant\" in full_text:\n",
    "#         generated_text = full_text.split(\"assistant\")[-1].strip()\n",
    "#     else:\n",
    "#         generated_text = full_text\n",
    "\n",
    "#     return generated_text\n",
    "\n",
    "# # test\n",
    "# prompt = \"What is C. elegans?\"\n",
    "# fluid_translate(prompt, lang='jp2en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use llama.cpp (faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from llama_cpp import Llama\n",
    "from contextlib import redirect_stderr\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def download_model(model_name,model_family,model_guuf_file):\n",
    "        with open(os.devnull, 'w') as devnull:\n",
    "            with redirect_stderr(devnull):\n",
    "                if not os.path.exists(model_guuf_file):\n",
    "                    url = f\"https://huggingface.co/LiquidAI/{model_family}/resolve/main/{model_name}\"\n",
    "                    download_gguf_model(url)\n",
    "\n",
    "def download_gguf_model(model_url: str, models_folder: str = \"../models\") -> str:\n",
    "    \"\"\"\n",
    "    Download a GGUF model file from a Hugging Face URL to the models folder.\n",
    "    \n",
    "    Args:\n",
    "        model_url: The HTTPS URL to the GGUF model file\n",
    "        models_folder: The destination folder (default: \"../models\")\n",
    "    \n",
    "    Returns:\n",
    "        str: The local path to the downloaded model file\n",
    "    \"\"\"\n",
    "    # Create models folder if it doesn't exist\n",
    "    Path(models_folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Extract filename from URL\n",
    "    filename = model_url.split(\"/\")[-1].split(\"?\")[0]\n",
    "    local_path = os.path.join(models_folder, filename)\n",
    "    \n",
    "    # Check if file already exists\n",
    "    if os.path.exists(local_path):\n",
    "        print(f\"[yellow]âš ï¸  Model already exists:[/yellow] [dim]{local_path}[/dim]\")\n",
    "        return local_path\n",
    "    \n",
    "    print(f\"[cyan]â¬‡ï¸  Downloading model from:[/cyan] [dim]{model_url}[/dim]\")\n",
    "    print(f\"[cyan]ðŸ“ Saving to:[/cyan] [dim]{local_path}[/dim]\")\n",
    "    \n",
    "    try:\n",
    "        # Stream download with progress\n",
    "        response = requests.get(model_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 8192\n",
    "        downloaded = 0\n",
    "        \n",
    "        with open(local_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=block_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    if total_size > 0:\n",
    "                        progress = (downloaded / total_size) * 100\n",
    "                        print(f\"\\r[cyan]Progress:[/cyan] {progress:.1f}% ({downloaded / (1024*1024):.1f} MB / {total_size / (1024*1024):.1f} MB)\", end=\"\")\n",
    "        \n",
    "        print(f\"\\n[green]âœ… Download complete![/green]\")\n",
    "        return local_path\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"\\n[red]âŒ Download failed: {e}[/red]\")\n",
    "        # Clean up partial download\n",
    "        if os.path.exists(local_path):\n",
    "            os.remove(local_path)\n",
    "        raise\n",
    "    \n",
    "# Suppress llama.cpp verbose output\n",
    "def load_llm(model_path, n_ctx=8000):\n",
    "    \"\"\"\n",
    "    Load a llama.cpp model with specified context window size.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the GGUF model file\n",
    "        n_ctx: Maximum context window size in tokens (default: 2048)\n",
    "               Common values: 512, 1024, 2048, 4096, 8192, 16384, 32768\n",
    "    \"\"\"\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        with redirect_stderr(devnull):\n",
    "            llm = Llama(\n",
    "                model_path=model_path,\n",
    "                n_ctx=n_ctx,\n",
    "                verbose=False,\n",
    "                temperature=0.1,\n",
    "            )\n",
    "            return llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract PII double step: General (1.2B) + Nano (1.2 EXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/unstructured_pii/tesseract_Redactor_sample_0001.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0002.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0003.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0004.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0005.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0006.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0007.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0008.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0009.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0010.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0011.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0012.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0013.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0014.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0015.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0016.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0017.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0018.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0019.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0020.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0021.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0022.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0023.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0024.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0025.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0026.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0027.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0028.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0029.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0030.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0031.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0032.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0033.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0034.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0035.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0036.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0037.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0038.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0039.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0040.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0041.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0042.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0043.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0044.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0045.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0046.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0047.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0048.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0049.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0050.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0051.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0052.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0053.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0054.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0055.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0056.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0057.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0058.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0059.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0060.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0061.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0062.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0063.txt\n",
      "../data/unstructured_pii/tesseract_Redactor_sample_0064.txt\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"You're are helpful assistant. Be concise and accurate.\"\n",
    "\n",
    "MODEL_NAME=\"LFM2-1.2B-Q8_0.gguf\"\n",
    "MODEL_GGUF_FILE = f\"../models/{MODEL_NAME}\"\n",
    "MODEL_FAMILY=\"LFM2-1.2B-GGUF\"\n",
    "\n",
    "def get_user_prompt_pii(contract):\n",
    "    return f\"\"\"\n",
    "Below is a text from a Japanese Legal contract. \n",
    "It contains Personal Identifiable information in Japanese.\n",
    "You are an expert in Japanese Personal Identifiable Information Detection.\n",
    "Your task it to \n",
    "1. carefully read the content identify Personal Information\n",
    "2. extract four type of personal identifiable information: such as person names, company names, addresses, and phone numbers\"\n",
    "\n",
    "---- Japanese Legal contract ----\n",
    "{contract}\n",
    "\"\"\"\n",
    "\n",
    "def find_pii(fn):\n",
    "    \"\"\"\n",
    "    Translate a Japanese text file to English and save as fn_en.txt\n",
    "    \n",
    "    Args:\n",
    "        fn: Path to the input text file\n",
    "    \"\"\"\n",
    "\n",
    "    with open(fn, 'r', encoding='utf-8') as f:\n",
    "        contract = f.read()\n",
    "    history = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "    history.append({\"role\": \"user\", \"content\": get_user_prompt_pii(contract)})\n",
    "    \n",
    "    model_id = \"LiquidAI/LFM2-350M-ENJP-MT\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    tokens = tokenizer.encode(str(history))\n",
    "    num_tokens = len(tokens)\n",
    "    _ = print(f\"Prompt has {num_tokens} tokens\")\n",
    "    \n",
    "    # Translate using fluid_translate\n",
    "    t0=time.time()\n",
    "    # Increase context window to handle larger contracts\n",
    "    llm_pii = load_llm(MODEL_GGUF_FILE, n_ctx=16000)\n",
    "    output = llm_pii.create_chat_completion(messages=history)\n",
    "    unstructured_pii = output[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    dt =time.time()-t0\n",
    "    _ = print(f\"took {dt} to extraxt pii {num_tokens} ({num_tokens/dt:.2f} token/sec)\")\n",
    "    return unstructured_pii\n",
    "\n",
    "unstructured_pii_folder = '../data/unstructured_pii/'\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(unstructured_pii_folder, exist_ok=True)\n",
    "\n",
    "txt_folder='../data/txt/'\n",
    "fns = glob.glob(f'{txt_folder}*.txt')\n",
    "fns = sorted([fn.split('/')[-1] for fn in fns if 'tesse' in fn ])\n",
    "\n",
    "for fn in fns:\n",
    "    output_path = os.path.join(unstructured_pii_folder, fn)\n",
    "    print(output_path)\n",
    "    if not os.path.exists(output_path):\n",
    "        unstructured_pii=find_pii(txt_folder+fn)\n",
    "        \n",
    "        # Save unstructured_pii text to unstructured_pii_folder/fn\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(unstructured_pii)\n",
    "        print(f\"Saved unstructured PII to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/json_pii/tesseract_Redactor_sample_0001.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0001.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0002.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0002.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0003.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0003.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0004.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0004.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0005.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0005.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0006.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0006.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0007.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0007.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0008.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0008.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0009.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0009.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0010.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0010.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0011.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0011.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0012.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0012.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0013.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0013.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0014.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0014.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0015.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0015.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0016.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0016.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0017.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0017.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0018.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0018.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0019.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0019.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0020.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0020.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0021.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0021.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0022.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0022.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0023.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0023.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0024.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0024.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0025.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0025.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0026.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0026.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0027.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0027.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0028.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0028.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0029.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0029.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0030.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0030.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0031.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0031.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0032.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0032.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0033.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0033.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0034.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0034.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0035.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0035.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0036.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved json_pii to: ../data/json_pii/tesseract_Redactor_sample_0036.json\n",
      "../data/json_pii/tesseract_Redactor_sample_0037.json\n"
     ]
    }
   ],
   "source": [
    "MODEL_GGUF_FILE_EXTRACT = \"../models/LFM2-1.2B-Extract-F16.gguf\"\n",
    "def get_user_prompt_extract(txt):\n",
    "    return f\"\"\"\n",
    "-----\n",
    "{txt}\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Identify and extract information matching the following schema. No other information has to be extracted. Adhere to the 4 fields schema, and to the exact 4 english key names\n",
    "\n",
    "\"full_name\": name of the person\" (string),\n",
    "\"company_name\": name of the company (string) ,\n",
    "\"address\": address of the company (string) ,\n",
    "\"phone_number\": phone number (string)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "unstructured_pii_folder = '../data/unstructured_pii/'\n",
    "json_pii = '../data/json_pii/'\n",
    "\n",
    "os.makedirs(json_pii, exist_ok=True)\n",
    "\n",
    "txt_folder='../data/txt/'\n",
    "fns = glob.glob(f'{unstructured_pii_folder}*.txt')\n",
    "fns = sorted([fn.split('/')[-1] for fn in fns if 'padd' in fn ])\n",
    "\n",
    "for fn in fns:\n",
    "    output_path = os.path.join(json_pii, fn).replace('.txt','.json')\n",
    "    print(output_path)\n",
    "    if True or not os.path.exists(output_path):\n",
    "        with open(unstructured_pii_folder+fn, 'r', encoding='utf-8') as f:\n",
    "            unstructured_pii = f.read()\n",
    "        history = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "        history.append({\"role\": \"user\", \"content\": get_user_prompt_extract(unstructured_pii)})\n",
    "        llm_extract = load_llm(MODEL_GGUF_FILE_EXTRACT)\n",
    "\n",
    "        output = llm_extract.create_chat_completion(messages=history)\n",
    "        pii = output[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        # Save unstructured_pii text to unstructured_pii_folder/fn\n",
    "\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(pii)\n",
    "        print(f\"Saved json_pii to: {output_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/LiquidAI/LFM2-350M-ENJP-MT-GGUF/resolve/main/LFM2-350M-ENJP-MT-F32.gguf?download=true\n",
    "\n",
    "MODEL_NAME=\"LFM2-350M-ENJP-MT-Q4_0.gguf\"\n",
    "MODEL_GGUF_FILE = f\"../models/{MODEL_NAME}\"\n",
    "MODEL_FAMILY=\"LFM2-350M-ENJP-MT-GGUF\"\n",
    "\n",
    "def fluid_translate(prompt, lang='jp2en'):\n",
    "    if lang == \"jp2en\":\n",
    "        system_prompt = \"Translate to English\"\n",
    "    elif lang == \"en2jp\":\n",
    "        system_prompt = \"Translate to Japanese\"\n",
    "    else:\n",
    "        raise Exception(\"lang not supported\")\n",
    "    download_model(MODEL_NAME,MODEL_FAMILY,MODEL_GGUF_FILE)\n",
    "    llm = load_llm(MODEL_GGUF_FILE)\n",
    "    history = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    output = llm.create_chat_completion(messages=history)\n",
    "    generated_message = output[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return generated_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jp2en(fn, override=False):\n",
    "    \"\"\"\n",
    "    Translate a Japanese text file to English and save as fn_en.txt\n",
    "    \n",
    "    Args:\n",
    "        fn: Path to the input text file\n",
    "    \"\"\"\n",
    "    base_name = os.path.splitext(fn)[0]\n",
    "    output_fn = f\"{base_name}_en.txt\"\n",
    "    if not os.path.exists(output_fn) or override:     \n",
    "        # Read the content of the file\n",
    "        with open(fn, 'r', encoding='utf-8') as f:\n",
    "            prompt = f.read()\n",
    "        \n",
    "        model_id = \"LiquidAI/LFM2-350M-ENJP-MT\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "        # Print how many tokens is the prompt\n",
    "        tokens = tokenizer.encode(prompt)\n",
    "        num_tokens = len(tokens)\n",
    "        print(f\"Prompt has {num_tokens} tokens\")\n",
    "        \n",
    "        # Translate using fluid_translate\n",
    "        t0=time.time()\n",
    "        translated_text = fluid_translate(prompt, lang='jp2en')\n",
    "        dt =time.time()-t0\n",
    "        print(f\"took {dt} to translate {num_tokens} ({num_tokens/dt:.2f} token/sec)\")\n",
    "        # Write translated text to file\n",
    "        with open(output_fn, 'w', encoding='utf-8') as f:\n",
    "            f.write(translated_text)\n",
    "        \n",
    "        print(f\"Translated {fn} -> {output_fn}\")\n",
    "        return output_fn\n",
    "\n",
    "\n",
    "fns = glob.glob('../data/txt/*.txt')\n",
    "fns = [fn for fn in fns if \"_en\" not in fn]\n",
    "fns\n",
    "for fn in fns:\n",
    "    jp2en(fn, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
